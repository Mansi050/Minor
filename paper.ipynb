{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dependencies\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing all global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_agents = 3  #LT,MT,ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_epochs = 50\n",
    "number_of_periods = 30 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List to store the global state at the end of each epoch\n",
    "global_state_per_epoch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#action space contains of reducing or increasing the amount of tons CO2 consumed per capita\n",
    "action_space = [\n",
    "    -0.2, -0.16, -0.12, -0.08, -0.04, 0, 0.04, 0.08, 0.12, 0.16, 0.2\n",
    "]\n",
    "size_of_action_space = len(action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_of_action = 5  #defining cost to reduce CO2 emissions per metric ton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-tables\n",
    "Q_LT = np.zeros((number_of_periods - 1, size_of_action_space))\n",
    "Q_MT = np.zeros((number_of_periods - 1, size_of_action_space))\n",
    "Q_ST = np.zeros((number_of_periods - 1, size_of_action_space))\n",
    "\n",
    "#Q-tables for every epoch that stores all max values from\n",
    "Q_LT_per_epoch = []\n",
    "Q_MT_per_epoch = []\n",
    "Q_ST_per_epoch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining best actions from Q-tables\n",
    "Max_Q_LT = 0\n",
    "Max_Q_MT = 0\n",
    "Max_Q_ST = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the weight factors of immediate rewards\n",
    "LT_reward_factor = 0.4\n",
    "MT_reward_factor = 0.5\n",
    "ST_reward_factor = 0.6\n",
    "cumulative_reward = 0  #initializing cumulative reward, which is 0 to start with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a list to store the cumulative reward for each epoch\n",
    "cumulative_reward_per_epoch = []\n",
    "\n",
    "#creating a list to store the immediate rewards for each epoch\n",
    "immediate_rewards_per_epoch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epsilons are initialized within epoch.\n",
    "\n",
    "#rate of decrement\n",
    "LT_epsilon_decay = 0.95\n",
    "MT_epsilon_decay = 0.9\n",
    "ST_epsilon_decay = 0.85\n",
    "\n",
    "#min value\n",
    "LT_epsilon_min = 0.1\n",
    "MT_epsilon_min = 0.06\n",
    "ST_epsilon_min = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha <=> Learning rate (will be initialized within epoch)\n",
    "alpha_min = 0.01  #min alpha\n",
    "alpha_decay = 0.995  #rate of decrement\n",
    "\n",
    "gamma = 0.7  #<=>reward discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 587
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3677,
     "status": "error",
     "timestamp": 1568473201788,
     "user": {
      "displayName": "Parth Agarwal",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCFQFb2g_oFfeuJQfc-PjClHPSuvaY1OMeVrWNFIg=s64",
      "userId": "10422930524292251625"
     },
     "user_tz": -330
    },
    "id": "0-72uedainsP",
    "outputId": "65ef1a05-bd9c-4633-9459-ba60a9aa8bdd"
   },
   "outputs": [],
   "source": [
    "#run epoch loop\n",
    "for epoch in range (1, number_of_epochs):\n",
    "  \n",
    "  #initializing variables which are reset for every new epoch\n",
    "  global_state = 4.025 #1984 CO2 emissions (metric tons per capita)\n",
    "  cumulative_reward = 0 #initializing cumulative reward, which is 0 to start with\n",
    "  alpha = 0.1 #initializing the learning rate of the Q-values\n",
    "  \n",
    "  #initializing epsilons for each agent, the variable to balance exploration and exploitation, higher epsilon means more exploitation\n",
    "  LT_epsilon = 0.9 \n",
    "  MT_epsilon = 0.8 \n",
    "  ST_epsilon = 0.7 \n",
    "  \n",
    "  #run period loop\n",
    "  for period in range (0, number_of_periods-2):\n",
    "    \n",
    "    #initializing variables\n",
    "    random_action = random.randrange(0,size_of_action_space)\n",
    "    LT_action = action_space[random_action]\n",
    "    random_action = random.randrange(0,size_of_action_space)\n",
    "    MT_action = action_space[random_action]\n",
    "    random_action = random.randrange(0,size_of_action_space)\n",
    "    ST_action = action_space[random_action]\n",
    "    \n",
    "    #Immediate Reward functions\n",
    "    LT_immediate_reward = -1 * LT_action * LT_reward_factor + cost_of_action * LT_action \n",
    "    MT_immediate_reward = -1 * MT_action * MT_reward_factor + cost_of_action * MT_action \n",
    "    ST_immediate_reward = -1 * ST_action * ST_reward_factor + cost_of_action * ST_action    \n",
    "    \n",
    "    #action iteration and Q-Table updates with exploration vs. exploitation\n",
    "    #LT\n",
    "    if np.random.rand() <= LT_epsilon:\n",
    "      random_action = random.randrange(0,size_of_action_space)\n",
    "      LT_action = action_space[random_action]\n",
    "      Q_LT[period, random_action] = round((1-alpha) * Q_LT[period, random_action] + alpha * (LT_immediate_reward + gamma * np.amax(Q_LT[period + 1, :])), 2)\n",
    "    else:\n",
    "      LT_action = action_space[Max_Q_LT]\n",
    "      Q_LT[period, Max_Q_LT] = round((1-alpha) * Q_LT[period, Max_Q_LT] + alpha * (LT_immediate_reward + gamma * np.amax(Q_LT[period + 1, :])), 2)\n",
    "    \n",
    "    #MT\n",
    "    if np.random.rand() <= MT_epsilon:\n",
    "      random_action = random.randrange(0,size_of_action_space)\n",
    "      MT_action = action_space[random_action]\n",
    "      Q_MT[period, random_action] = round((1-alpha) * Q_MT[period, random_action] + alpha * (MT_immediate_reward + gamma * np.amax(Q_MT[period + 1, :])), 2)\n",
    "    else:\n",
    "      MT_action = action_space[Max_Q_MT]\n",
    "      Q_MT[period, Max_Q_MT] = round((1-alpha) * Q_MT[period, Max_Q_MT] + alpha * (LT_immediate_reward + gamma * np.amax(Q_MT[period + 1, :])), 2)\n",
    "    \n",
    "    #ST\n",
    "    if np.random.rand() <= ST_epsilon:\n",
    "      random_action = random.randrange(0,size_of_action_space)\n",
    "      ST_action = action_space[random_action]\n",
    "      Q_ST[period, random_action] = round((1-alpha) * Q_ST[period, random_action] + alpha * (ST_immediate_reward + gamma * np.amax(Q_ST[period + 1, :])), 2)\n",
    "    else:\n",
    "      ST_action = action_space[Max_Q_ST]\n",
    "      Q_ST[period, Max_Q_ST] = round((1-alpha) * Q_ST[period, Max_Q_ST] + alpha * (ST_immediate_reward + gamma * np.amax(Q_ST[period + 1, :])), 2)\n",
    "    \n",
    "    #to be maximized: += immediate rewards of both agents + the negative of the actions that each agent takes, because the actions influence the global state, which shall be minimized\n",
    "    cumulative_reward += LT_immediate_reward + MT_immediate_reward + ST_immediate_reward - 5*(action_space[np.argmax(Q_LT[period, :])]) - 5*(action_space[np.argmax(Q_MT[period, :])]) - 5*(action_space[np.argmax(Q_ST[period, :])])\n",
    "    \n",
    "    #decaying learning rate and agent's epsilon values\n",
    "    alpha = alpha*alpha_decay if (alpha > alpha_min) else alpha\n",
    "    LT_epsilon = LT_epsilon*LT_epsilon_decay if (LT_epsilon > LT_epsilon_min) else LT_epsilon\n",
    "    MT_epsilon = MT_epsilon*MT_epsilon_decay if (MT_epsilon > MT_epsilon_min) else MT_epsilon\n",
    "    ST_epsilon = ST_epsilon*ST_epsilon_decay if (ST_epsilon > ST_epsilon_min) else ST_epsilon\n",
    "    \n",
    "    #udpating global state and period counter\n",
    "    global_state += (LT_action + MT_action + ST_action) / number_of_agents\n",
    "\n",
    "#for each epoch, store the policies of each agent\n",
    "  Q_LT_per_epoch.append(np.argmax(Q_LT, axis=1).tolist())\n",
    "  Q_MT_per_epoch.append(np.argmax(Q_MT, axis=1).tolist())\n",
    "  Q_ST_per_epoch.append(np.argmax(Q_ST, axis=1).tolist())\n",
    "  \n",
    "  #store the cumulative and immediate rewards and the global states of each epoch\n",
    "  cumulative_reward_per_epoch.append(cumulative_reward)\n",
    "  immediate_rewards_per_epoch.append((LT_immediate_reward, MT_immediate_reward, ST_immediate_reward))\n",
    "  global_state_per_epoch.append(global_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Best Epoch based on Cumulative Reward: 0\n",
      "Highest Cumulative Reward: 85.82\n",
      "Best Epoch based on Global State: 10\n",
      "Lowest Global State: -0.92\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-646f52045aa3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m print(\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;34mf\"Best Epoch based on LT's Immediate Reward: {np.argmax(immediate_rewards_per_epoch[:, 0])}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m print(\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "#evaluating the trained model\n",
    "print('\\n')\n",
    "print(\n",
    "    f\"Best Epoch based on Cumulative Reward: {np.argmax(cumulative_reward_per_epoch)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Highest Cumulative Reward: {round(np.amax(cumulative_reward_per_epoch), 2)}\"\n",
    ")\n",
    "print(f\"Best Epoch based on Global State: {np.argmin(global_state_per_epoch)}\")\n",
    "print(f\"Lowest Global State: {round(np.min(global_state_per_epoch), 2)}\")\n",
    "print('\\n')\n",
    "print(\n",
    "    f\"Best Epoch based on LT's Immediate Reward: {np.argmax(immediate_rewards_per_epoch[:, 0])}\"\n",
    ")\n",
    "print(\n",
    "    f\"Highest Immediate Reward for LT: {round(np.max(immediate_rewards_per_epoch[:, 0]), 2)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Best Epoch based on MT's Immediate Reward: {np.argmax(immediate_rewards_per_epoch[:, 1])}\"\n",
    ")\n",
    "print(\n",
    "    f\"Highest Immediate Reward for MT: {round(np.max(immediate_rewards_per_epoch[:, 1]), 2)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Best Epoch based on ST's Immediate Reward: {np.argmax(immediate_rewards_per_epoch[:, 2])}\"\n",
    ")\n",
    "print(\n",
    "    f\"Highest Immediate Reward for ST: {round(np.max(immediate_rewards_per_epoch[:, 2]), 2)}\"\n",
    ")\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-183499440b3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#build utilitarian, selfish and greedy policies of each agent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m Q_LT_Best = Q_LT_per_epoch[np.argmax(\n\u001b[1;32m----> 3\u001b[1;33m     cumulative_reward_per_epoch[:, 1]).tolist()]\n\u001b[0m\u001b[0;32m      4\u001b[0m Q_MT_Best = Q_MT_per_epoch[np.argmax(\n\u001b[0;32m      5\u001b[0m     cumulative_reward_per_epoch[:, 1]).tolist()]\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "#build utilitarian, selfish and greedy policies of each agent\n",
    "Q_LT_Best = Q_LT_per_epoch[np.argmax(cumulative_reward_per_epoch[:, 1]).tolist()]\n",
    "Q_MT_Best = Q_MT_per_epoch[np.argmax(cumulative_reward_per_epoch[:, 1]).tolist()]\n",
    "Q_ST_Best = Q_ST_per_epoch[np.argmax(cumulative_reward_per_epoch[:, 1]).tolist()]\n",
    "\n",
    "Q_LT_Immediate_Best = Q_LT_per_epoch[np.argmax(immediate_rewards_per_epoch[:, 1]).tolist()]\n",
    "Q_MT_Immediate_Best = Q_MT_per_epoch[np.argmax(immediate_rewards_per_epoch[:, 1]).tolist()]\n",
    "Q_ST_Immediate_Best = Q_ST_per_epoch[np.argmax(immediate_rewards_per_epoch[:, 2]).tolist()]\n",
    "                                                       \n",
    "LT_Strategy = [action_space[i] for i in Q_LT_Best.astype(int)[1:]]\n",
    "MT_Strategy = [action_space[i] for i in Q_MT_Best.astype(int)[1:]]\n",
    "ST_Strategy = [action_space[i] for i in Q_ST_Best.astype(int)[1:]]\n",
    "\n",
    "LT_Greedy_Strategy = [action_space[i] for i in Q_LT_Immediate_Best.astype(int)[1:]]\n",
    "MT_Greedy_Strategy = [action_space[i] for i in Q_MT_Immediate_Best.astype(int)[1:]]\n",
    "ST_Greedy_Strategy = [action_space[i] for i in Q_ST_Immediate_Best.astype(int)[1:]]\n",
    "\n",
    "LT_Policy = [action_space[i] for i in np.argmax(Q_LT, axis=1)]\n",
    "MT_Policy = [action_space[i] for i in np.argmax(Q_MT, axis=1)]\n",
    "ST_Policy = [action_space[i] for i in np.argmax(Q_ST, axis=1)]\n",
    "\n",
    "print(f\"LT's Strategy to achieve Highest Cumulative Reward: \\n {LT_Strategy}\")\n",
    "print(f\"LT's Strategy to achieve Highest Immediate Reward: \\n {LT_Greedy_Strategy}\")\n",
    "print(f\"Selfish Policy of LT, based on LT's Final Q-Table: \\n {LT_Policy}\")\n",
    "print('\\n')\n",
    "\n",
    "print(f\"MT's Strategy to achieve Highest Cumulative Reward: \\n {MT_Strategy}\")\n",
    "print(f\"MT's Strategy to achieve Highest Immediate Reward: \\n {MT_Greedy_Strategy}\")\n",
    "print(f\"Selfish Policy of MT, based on MT's Final Q-Table: \\n {MT_Policy}\")\n",
    "print('\\n')\n",
    "\n",
    "print(f\"ST's Strategy to achieve Highest Cumulative Reward: \\n {ST_Strategy}\")\n",
    "print(f\"ST's Strategy to achieve Highest Immediate Reward: \\n {ST_Greedy_Strategy}\")\n",
    "print(f\"Selfish Policy of ST, based on ST's Final Q-Table: \\n {ST_Policy}\")\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'action_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-f8a5e0cad757>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Final Q-Table of LT: \\n {action_count}\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Q-Tables are printed...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_rowcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_LT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnumber_of_periods\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'action_count' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Final Q-Table of LT: \\n {action_count}\")  #Q-Tables are printed...\n",
    "print(Q_rowcount(Q_LT[:number_of_periods - 2]))\n",
    "print('\\n')\n",
    "\n",
    "print(f\"Final Q-Table of MT: \\n {action_count}\")  #Q-Tables are printed...\n",
    "print(Q_rowcount(Q_MT[:number_of_periods - 2]))\n",
    "print('\\n')\n",
    "\n",
    "print(\n",
    "    f\"Final Q-Table of ST: \\n {action_count}\"\n",
    ")  #...Based on the \"rewards_per_epoch\" Table, the best Q-Tables are identified as policies\n",
    "print(Q_rowcount(Q_ST[:number_of_periods - 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Cumulative Reward under Selfish Policies: 23.22\n",
      "Global State after using Selfish Policies: 0.01\n",
      "\n",
      "\n",
      "Percentage of Lower Cumulative Reward comparing Selfish Policies to Best Epoch: -66.93%\n",
      "Percentage of Higher Global State comparing Selfish Policies to Best Epoch: -100.97%\n"
     ]
    }
   ],
   "source": [
    "#Under utilary Policies\n",
    "#initializing variables which are reset for every new epoch\n",
    "global_state = 4.97  #current CO2 emissions are at 4.97 metric tons per capita world-wide. Source: World Bank\n",
    "cumulative_reward_selfish = 0  #initializing cumulative reward, which is 0 to start with\n",
    "alpha = 0.1  #initializing the learning rate of the Q-values\n",
    "Q_LT = np.zeros((number_of_periods - 1, size_of_action_space))\n",
    "Q_MT = np.zeros((number_of_periods - 1, size_of_action_space))\n",
    "Q_ST = np.zeros((number_of_periods - 1, size_of_action_space))\n",
    "\n",
    "#run period loop\n",
    "for period in range(0, number_of_periods - 2):\n",
    "\n",
    "    #actions based on Policies\n",
    "    LT_action = ST_Strategy[period]\n",
    "    MT_action = MT_Strategy[period]\n",
    "    ST_action = ST_Strategy[period]\n",
    "\n",
    "    #Immediate Reward functions\n",
    "    LT_immediate_reward = -1 * LT_action * LT_reward_factor + cost_of_action * LT_action  #defining immediate reward function of LT per period\n",
    "    MT_immediate_reward = -1 * LT_action * MT_reward_factor + cost_of_action * MT_action  #defining immediate reward function of LT per period\n",
    "    ST_immediate_reward = -1 * ST_action * ST_reward_factor + cost_of_action * ST_action  #defining immediate reward function of ST per period\n",
    "\n",
    "    #to be maximized: += immediate rewards of both agents + the negative of the actions that each agent takes, because the actions influence the global state, which shall be minimized\n",
    "    cumulative_reward_selfish += LT_immediate_reward + MT_immediate_reward + ST_immediate_reward - 5 * (\n",
    "        action_space[np.argmax(Q_LT[period, :])]\n",
    "    ) - 5 * (action_space[np.argmax(\n",
    "        Q_MT[period, :])]) - 5 * (action_space[np.argmax(Q_ST[period, :])])\n",
    "\n",
    "    global_state += (LT_action + MT_action + ST_action) / number_of_agents\n",
    "    period += 1\n",
    "\n",
    "print('\\n')\n",
    "print(\n",
    "    f\"Cumulative Reward under Selfish Policies: {round(cumulative_reward_selfish, 2)}\"\n",
    ")\n",
    "print(f\"Global State after using Selfish Policies: {round(global_state, 2)}\")\n",
    "\n",
    "Selfish_Reward_Loss = round(\n",
    "    100 *\n",
    "    (cumulative_reward_selfish - np.max(cumulative_reward_per_epoch[:, 1])) /\n",
    "    np.max(cumulative_reward_per_epoch[:, 1]), 2)\n",
    "CO2_Selfish = round(\n",
    "    100 * ((global_state - np.min(global_state_per_epoch[:, 1])) /\n",
    "           np.min(global_state_per_epoch[:, 1])), 2)\n",
    "\n",
    "print('\\n')\n",
    "print(\n",
    "    f\"Percentage of Lower Cumulative Reward comparing Selfish Policies to Best Epoch: {Selfish_Reward_Loss}%\"\n",
    ")\n",
    "print(\n",
    "    f\"Percentage of Higher Global State comparing Selfish Policies to Best Epoch: {CO2_Selfish}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Cumulative Reward under Selfish Policies: 61.9\n",
      "Global State after using Selfish Policies: 2.9\n",
      "\n",
      "\n",
      "Percentage of Lower Cumulative Reward comparing Selfish Policies to Best Epoch: -11.84%\n",
      "Percentage of Higher Global State comparing Selfish Policies to Best Epoch: -382.33%\n"
     ]
    }
   ],
   "source": [
    "#Under Selfish Policies\n",
    "#initializing variables which are reset for every new epoch\n",
    "global_state = 4.97  #current CO2 emissions are at 4.97 metric tons per capita world-wide. Source: World Bank\n",
    "cumulative_reward_selfish = 0  #initializing cumulative reward, which is 0 to start with\n",
    "alpha = 0.1  #initializing the learning rate of the Q-values\n",
    "Q_LT = np.zeros((number_of_periods - 1, size_of_action_space))\n",
    "Q_MT = np.zeros((number_of_periods - 1, size_of_action_space))\n",
    "Q_ST = np.zeros((number_of_periods - 1, size_of_action_space))\n",
    "\n",
    "#run period loop\n",
    "for period in range(0, number_of_periods - 2):\n",
    "\n",
    "    #actions based on Policies\n",
    "    LT_action = LT_Policy[period]\n",
    "    MT_action = MT_Policy[period]\n",
    "    ST_action = ST_Policy[period]\n",
    "\n",
    "    #Immediate Reward functions\n",
    "    LT_immediate_reward = -1 * LT_action * LT_reward_factor + cost_of_action * LT_action  #defining immediate reward function of LT per period\n",
    "    MT_immediate_reward = -1 * LT_action * MT_reward_factor + cost_of_action * MT_action  #defining immediate reward function of LT per period\n",
    "    ST_immediate_reward = -1 * ST_action * ST_reward_factor + cost_of_action * ST_action  #defining immediate reward function of ST per period\n",
    "\n",
    "    #to be maximized: += immediate rewards of both agents + the negative of the actions that each agent takes, because the actions influence the global state, which shall be minimized\n",
    "    cumulative_reward_selfish += LT_immediate_reward + MT_immediate_reward + ST_immediate_reward - 5 * (\n",
    "        action_space[np.argmax(Q_LT[period, :])]\n",
    "    ) - 5 * (action_space[np.argmax(\n",
    "        Q_MT[period, :])]) - 5 * (action_space[np.argmax(Q_ST[period, :])])\n",
    "\n",
    "    global_state += (LT_action + MT_action + ST_action) / number_of_agents\n",
    "    period += 1\n",
    "\n",
    "print('\\n')\n",
    "print(\n",
    "    f\"Cumulative Reward under Selfish Policies: {round(cumulative_reward_selfish, 2)}\"\n",
    ")\n",
    "print(f\"Global State after using Selfish Policies: {round(global_state, 2)}\")\n",
    "\n",
    "Selfish_Reward_Loss = round(\n",
    "    100 *\n",
    "    (cumulative_reward_selfish - np.max(cumulative_reward_per_epoch[:, 1])) /\n",
    "    np.max(cumulative_reward_per_epoch[:, 1]), 2)\n",
    "CO2_Selfish = round(\n",
    "    100 * ((global_state - np.min(global_state_per_epoch[:, 1])) /\n",
    "           np.min(global_state_per_epoch[:, 1])), 2)\n",
    "\n",
    "print('\\n')\n",
    "print(\n",
    "    f\"Percentage of Lower Cumulative Reward comparing Selfish Policies to Best Epoch: {Selfish_Reward_Loss}%\"\n",
    ")\n",
    "print(\n",
    "    f\"Percentage of Higher Global State comparing Selfish Policies to Best Epoch: {CO2_Selfish}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Cumulative Reward under Greedy Policies: 30.44\n",
      "Global State after using Greedy Policies: 0.6\n",
      "\n",
      "\n",
      "Percentage of Lower Cumulative Reward comparing Greedy Policies to Best Epoch: -61.68%\n",
      "Percentage of Higher Global State comparing Greedy Policies to Best Epoch: -155.85%\n"
     ]
    }
   ],
   "source": [
    "#Under Greedy Policies\n",
    "#initializing variables which are reset for every new epoch\n",
    "global_state = 4.97  #current CO2 emissions are at 4.97 metric tons per capita world-wide. Source: World Bank\n",
    "cumulative_reward_greedy = 0  #initializing cumulative reward, which is 0 to start with\n",
    "alpha = 0.1  #initializing the learning rate of the Q-values\n",
    "\n",
    "#run period loop\n",
    "for period in range(0, number_of_periods - 2):\n",
    "\n",
    "    #actions based on Policies\n",
    "    LT_action = LT_Greedy_Strategy[period]\n",
    "    MT_action = MT_Greedy_Strategy[period]\n",
    "    ST_action = ST_Greedy_Strategy[period]\n",
    "\n",
    "    #Immediate Reward functions\n",
    "    LT_immediate_reward = -1 * LT_action * LT_reward_factor + cost_of_action * LT_action\n",
    "    MT_immediate_reward = -1 * LT_action * MT_reward_factor + cost_of_action * MT_action\n",
    "    ST_immediate_reward = -1 * ST_action * ST_reward_factor + cost_of_action * ST_action\n",
    "\n",
    "    #to be maximized: += immediate rewards of both agents + the negative of the actions that each agent takes, because the actions influence the global state, which shall be minimized\n",
    "    cumulative_reward_greedy += LT_immediate_reward + MT_immediate_reward + ST_immediate_reward - 5 * (\n",
    "        action_space[np.argmax(Q_LT[period, :])]\n",
    "    ) - 5 * (action_space[np.argmax(\n",
    "        Q_MT[period, :])]) - 5 * (action_space[np.argmax(Q_ST[period, :])])\n",
    "\n",
    "    global_state += (LT_action + MT_action + ST_action) / number_of_agents\n",
    "    period += 1\n",
    "\n",
    "print('\\n')\n",
    "print(\n",
    "    f\"Cumulative Reward under Greedy Policies: {round(cumulative_reward_greedy, 2)}\"\n",
    ")\n",
    "print(f\"Global State after using Greedy Policies: {round(global_state, 2)}\")\n",
    "\n",
    "Greedy_Reward_Loss = round(\n",
    "    100 *\n",
    "    (cumulative_reward_greedy - np.max(cumulative_reward_per_epoch[:, 1])) /\n",
    "    np.max(cumulative_reward_per_epoch[:, 1]), 2)\n",
    "CO2_Greedy = round(\n",
    "    100 * ((global_state - np.min(global_state_per_epoch[:, 1])) /\n",
    "           np.min(global_state_per_epoch[:, 1])), 2)\n",
    "\n",
    "print('\\n')\n",
    "print(\n",
    "    f\"Percentage of Lower Cumulative Reward comparing Greedy Policies to Best Epoch: {Greedy_Reward_Loss}%\"\n",
    ")\n",
    "print(\n",
    "    f\"Percentage of Higher Global State comparing Greedy Policies to Best Epoch: {CO2_Greedy}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing period numbers in first column of Q-table\n",
    "def Q_rowcount(Q_Table):\n",
    "    subcount = 1\n",
    "    for row in Q_Table:\n",
    "        count = f\"p{subcount}\"\n",
    "        print(count, row)\n",
    "        subcount += 1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Dynamic Paper_MARL & Global Carbon Footprint_School of AI.ipynb.ipynb",
   "provenance": [
    {
     "file_id": "1wOS8w3V6DoSO-c46Pbq-OUr7kQl8cFl9",
     "timestamp": 1568473104371
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
