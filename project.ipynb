{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libs\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1985</th>\n",
       "      <th>1986</th>\n",
       "      <th>1987</th>\n",
       "      <th>1988</th>\n",
       "      <th>1989</th>\n",
       "      <th>1990</th>\n",
       "      <th>1991</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>...</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.076982</td>\n",
       "      <td>0.073146</td>\n",
       "      <td>0.076875</td>\n",
       "      <td>0.080231</td>\n",
       "      <td>0.080052</td>\n",
       "      <td>0.074072</td>\n",
       "      <td>0.071745</td>\n",
       "      <td>0.073260</td>\n",
       "      <td>0.073215</td>\n",
       "      <td>0.074249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081482</td>\n",
       "      <td>0.078695</td>\n",
       "      <td>0.079444</td>\n",
       "      <td>0.079535</td>\n",
       "      <td>0.075231</td>\n",
       "      <td>0.074109</td>\n",
       "      <td>0.073722</td>\n",
       "      <td>0.070257</td>\n",
       "      <td>0.069479</td>\n",
       "      <td>0.071449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.025159</td>\n",
       "      <td>0.028201</td>\n",
       "      <td>0.032614</td>\n",
       "      <td>0.029712</td>\n",
       "      <td>0.036319</td>\n",
       "      <td>0.050134</td>\n",
       "      <td>0.051584</td>\n",
       "      <td>0.057166</td>\n",
       "      <td>0.058675</td>\n",
       "      <td>0.052958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097371</td>\n",
       "      <td>0.106728</td>\n",
       "      <td>0.107960</td>\n",
       "      <td>0.109923</td>\n",
       "      <td>0.107925</td>\n",
       "      <td>0.113627</td>\n",
       "      <td>0.113551</td>\n",
       "      <td>0.114614</td>\n",
       "      <td>0.117321</td>\n",
       "      <td>0.121645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.057198</td>\n",
       "      <td>0.061475</td>\n",
       "      <td>0.062880</td>\n",
       "      <td>0.062228</td>\n",
       "      <td>0.062439</td>\n",
       "      <td>0.059846</td>\n",
       "      <td>0.061742</td>\n",
       "      <td>0.061078</td>\n",
       "      <td>0.062800</td>\n",
       "      <td>0.064839</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079657</td>\n",
       "      <td>0.078875</td>\n",
       "      <td>0.081551</td>\n",
       "      <td>0.086172</td>\n",
       "      <td>0.080837</td>\n",
       "      <td>0.091557</td>\n",
       "      <td>0.094971</td>\n",
       "      <td>0.100685</td>\n",
       "      <td>0.106954</td>\n",
       "      <td>0.111544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.072266</td>\n",
       "      <td>0.078686</td>\n",
       "      <td>0.080705</td>\n",
       "      <td>0.079294</td>\n",
       "      <td>0.080649</td>\n",
       "      <td>0.084033</td>\n",
       "      <td>0.073463</td>\n",
       "      <td>0.074652</td>\n",
       "      <td>0.080304</td>\n",
       "      <td>0.080940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080507</td>\n",
       "      <td>0.081097</td>\n",
       "      <td>0.079449</td>\n",
       "      <td>0.071548</td>\n",
       "      <td>0.069084</td>\n",
       "      <td>0.066095</td>\n",
       "      <td>0.063184</td>\n",
       "      <td>0.060153</td>\n",
       "      <td>0.062863</td>\n",
       "      <td>0.064928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.049170</td>\n",
       "      <td>0.050382</td>\n",
       "      <td>0.057310</td>\n",
       "      <td>0.051338</td>\n",
       "      <td>0.043277</td>\n",
       "      <td>0.046501</td>\n",
       "      <td>0.053381</td>\n",
       "      <td>0.068414</td>\n",
       "      <td>0.062216</td>\n",
       "      <td>0.067075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085228</td>\n",
       "      <td>0.090190</td>\n",
       "      <td>0.086500</td>\n",
       "      <td>0.087039</td>\n",
       "      <td>0.099709</td>\n",
       "      <td>0.104405</td>\n",
       "      <td>0.113078</td>\n",
       "      <td>0.111316</td>\n",
       "      <td>0.115982</td>\n",
       "      <td>0.093622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       1985      1986      1987      1988      1989      1990      1991  \\\n",
       "0  0.076982  0.073146  0.076875  0.080231  0.080052  0.074072  0.071745   \n",
       "1  0.025159  0.028201  0.032614  0.029712  0.036319  0.050134  0.051584   \n",
       "2  0.057198  0.061475  0.062880  0.062228  0.062439  0.059846  0.061742   \n",
       "3  0.072266  0.078686  0.080705  0.079294  0.080649  0.084033  0.073463   \n",
       "4  0.049170  0.050382  0.057310  0.051338  0.043277  0.046501  0.053381   \n",
       "\n",
       "       1992      1993      1994  ...      2005      2006      2007      2008  \\\n",
       "0  0.073260  0.073215  0.074249  ...  0.081482  0.078695  0.079444  0.079535   \n",
       "1  0.057166  0.058675  0.052958  ...  0.097371  0.106728  0.107960  0.109923   \n",
       "2  0.061078  0.062800  0.064839  ...  0.079657  0.078875  0.081551  0.086172   \n",
       "3  0.074652  0.080304  0.080940  ...  0.080507  0.081097  0.079449  0.071548   \n",
       "4  0.068414  0.062216  0.067075  ...  0.085228  0.090190  0.086500  0.087039   \n",
       "\n",
       "       2009      2010      2011      2012      2013      2014  \n",
       "0  0.075231  0.074109  0.073722  0.070257  0.069479  0.071449  \n",
       "1  0.107925  0.113627  0.113551  0.114614  0.117321  0.121645  \n",
       "2  0.080837  0.091557  0.094971  0.100685  0.106954  0.111544  \n",
       "3  0.069084  0.066095  0.063184  0.060153  0.062863  0.064928  \n",
       "4  0.099709  0.104405  0.113078  0.111316  0.115982  0.093622  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading dataFrames\n",
    "LT_df = pd.read_csv('LT_db.csv', index_col=0)\n",
    "MT_df = pd.read_csv('MT_db.csv', index_col=0)\n",
    "ST_df = pd.read_csv('ST_db.csv', index_col=0)\n",
    "LT_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = len(LT_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User Inputs\n",
    "epochs = 50\n",
    "cost_of_action = 50\n",
    "size_of_action_space = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "LT_values = LT_df.values[:-1].T\n",
    "Population_LT = LT_df.values[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LT_action_space:  [0.035 0.057 0.079 0.101 0.122 0.144 0.166 0.188 0.209 0.231]\n"
     ]
    }
   ],
   "source": [
    "LT_action_space = np.round(\n",
    "    np.linspace(min(LT_values.ravel()), max(LT_values.ravel()),\n",
    "                size_of_action_space + 2), 3)\n",
    "LT_action_space = LT_action_space[1:-1]\n",
    "print('LT_action_space: ', LT_action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "MT_values = MT_df.values[:-1].T\n",
    "Population_MT = MT_df.values[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MT_action_space:  [0.015 0.022 0.029 0.036 0.044 0.051 0.058 0.066 0.073 0.08 ]\n"
     ]
    }
   ],
   "source": [
    "MT_action_space = np.round(\n",
    "    np.linspace(min(MT_values.ravel()), max(MT_values.ravel()),\n",
    "                size_of_action_space + 2), 3)\n",
    "MT_action_space = MT_action_space[1:-1]\n",
    "print('MT_action_space: ', MT_action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_values = ST_df.values[:-1].T\n",
    "Population_ST = ST_df.values[-1]\n",
    "Population_array = Population_LT + Population_MT + Population_ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ST_action_space:  [0.008 0.015 0.022 0.029 0.036 0.043 0.05  0.057 0.064 0.071]\n"
     ]
    }
   ],
   "source": [
    "ST_action_space = np.round(\n",
    "    np.linspace(min(ST_values.ravel()), max(ST_values.ravel()),\n",
    "                size_of_action_space + 2), 3)\n",
    "ST_action_space = ST_action_space[1:-1]\n",
    "print('ST_action_space: ', ST_action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-tables\n",
    "Q_LT = np.zeros((years + 1, size_of_action_space))\n",
    "Q_MT = np.zeros((years + 1, size_of_action_space))\n",
    "Q_ST = np.zeros((years + 1, size_of_action_space))\n",
    "\n",
    "#Q-tables for every epoch that stores all max values from\n",
    "Q_LT_per_epoch = []\n",
    "Q_MT_per_epoch = []\n",
    "Q_ST_per_epoch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_state_per_epoch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the weight factors of immediate rewards\n",
    "LT_reward_factor = 0.4\n",
    "MT_reward_factor = 0.5\n",
    "ST_reward_factor = 0.6\n",
    "cumulative_reward = 0  #initializing cumulative reward, which is 0 to start with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a list to store the cumulative reward for each epoch\n",
    "cumulative_reward_per_epoch = []\n",
    "\n",
    "#creating a list to store the immediate rewards for each epoch\n",
    "immediate_rewards_per_epoch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epsilons are initialized within epoch.\n",
    "\n",
    "#rate of decrement\n",
    "LT_epsilon_decay = 0.95\n",
    "MT_epsilon_decay = 0.9\n",
    "ST_epsilon_decay = 0.85\n",
    "\n",
    "#min value\n",
    "LT_epsilon_min = 0.1\n",
    "MT_epsilon_min = 0.06\n",
    "ST_epsilon_min = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha <=> Learning rate (will be initialized within epoch)\n",
    "alpha_min = 0.01  #min alpha\n",
    "alpha_decay = 0.995  #rate of decrement\n",
    "\n",
    "gamma = 0.7  #<=>reward discount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining best actions from action space\n",
    "Max_Q_LT = 0\n",
    "Max_Q_MT = 0\n",
    "Max_Q_ST = 0\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "\n",
    "    #initalize variables\n",
    "    global_state_of_co2_emission = 4.025  #CO2 emmision in 1984\n",
    "\n",
    "    cumulative_reward = 0  #intially 0\n",
    "\n",
    "    alpha = 0.1  #learning rate\n",
    "    LT_epsilon = 0.9\n",
    "    MT_epsilon = 0.8\n",
    "    ST_epsilon = 0.7\n",
    "\n",
    "    for year in range(0, years):\n",
    "\n",
    "        #LT\n",
    "        if np.random.rand() <= LT_epsilon:\n",
    "            LT_action = random.choice(LT_values[year])\n",
    "        else:\n",
    "            LT_action = LT_action_space[Max_Q_LT]\n",
    "\n",
    "        LT_immediate_reward = -1 * LT_action * LT_reward_factor + cost_of_action * LT_action\n",
    "        Q_LT[year, absw(LT_action_space - LT_action).argmin()] = round(\n",
    "            (1 - alpha) * Q_LT[year, Max_Q_LT] + alpha *\n",
    "            (LT_immediate_reward + gamma * np.amax(Q_LT[year + 1, :])), 3)\n",
    "        \n",
    "        #MT\n",
    "        if np.random.rand() <= MT_epsilon:\n",
    "            MT_action = random.choice(MT_values[year])\n",
    "        else:\n",
    "            MT_action = MT_action_space[Max_Q_MT]\n",
    "\n",
    "        MT_immediate_reward = -1 * MT_action * MT_reward_factor + cost_of_action * MT_action\n",
    "        Q_MT[year, abs(MT_action_space - MT_action).argmin()] = round(\n",
    "            (1 - alpha) * Q_MT[year, Max_Q_MT] + alpha *\n",
    "            (MT_immediate_reward + gamma * np.amax(Q_MT[year + 1, :])), 3)\n",
    "        \n",
    "        #ST\n",
    "        if np.random.rand() <= ST_epsilon:\n",
    "            ST_action = random.choice(ST_values[year])\n",
    "        else:\n",
    "            ST_action = ST_action_space[Max_Q_ST]\n",
    "\n",
    "        ST_immediate_reward = -1 * ST_action * ST_reward_factor + cost_of_action * ST_action\n",
    "        Q_ST[year, abs(ST_action_space - ST_action).argmin()] = round(\n",
    "            (1 - alpha) * Q_ST[year, Max_Q_ST] + alpha *\n",
    "            (ST_immediate_reward + gamma * np.amax(Q_ST[year + 1, :])), 3)\n",
    "        \n",
    "        #to be maximized: += immediate rewards of both agents + the negative of the actions that each agent takes, because the actions influence the global state, which shall be minimized\n",
    "        cumulative_reward += LT_immediate_reward + MT_immediate_reward + ST_immediate_reward - cost_of_action*((LT_action_space[np.argmax(Q_LT[year, :])]) - (MT_action_space[np.argmax(Q_MT[year, :])]) - (ST_action_space[np.argmax(Q_ST[year, :])]))\n",
    "        \n",
    "        #decaying learning rate and agent's epsilon values\n",
    "        alpha = alpha*alpha_decay if (alpha > alpha_min) else alpha\n",
    "        LT_epsilon = LT_epsilon*LT_epsilon_decay if (LT_epsilon > LT_epsilon_min) else LT_epsilon\n",
    "        MT_epsilon = MT_epsilon*MT_epsilon_decay if (MT_epsilon > MT_epsilon_min) else MT_epsilon\n",
    "        ST_epsilon = ST_epsilon*ST_epsilon_decay if (ST_epsilon > ST_epsilon_min) else ST_epsilon\n",
    "        \n",
    "        global_state_of_co2_emission += ((LT_action*Population_LT[year]) + (MT_action*Population_MT[year]) + (ST_action*Population_ST[year]))/Population_array[year]\n",
    "    \n",
    "    \n",
    "    Q_LT_per_epoch.append(np.argmax(Q_LT, axis=1).tolist())\n",
    "    Q_MT_per_epoch.append(np.argmax(Q_MT, axis=1).tolist())\n",
    "    Q_ST_per_epoch.append(np.argmax(Q_ST, axis=1).tolist())\n",
    "  \n",
    "    cumulative_reward_per_epoch.append(cumulative_reward)\n",
    "    immediate_rewards_per_epoch.append([LT_immediate_reward, MT_immediate_reward, ST_immediate_reward])\n",
    "    global_state_per_epoch.append(global_state_of_co2_emission)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "immediate_rewards_per_epoch = np.array(immediate_rewards_per_epoch,copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Best Epoch based on Cumulative Reward:  18\n",
      "Highest Cumulative Reward: 106.14\n",
      "Best Epoch based on Global State: 6\n",
      "Lowest Global State: 4.43\n",
      "\n",
      "\n",
      "Best Epoch based on LT's Immediate Reward: 28\n",
      "Highest Immediate Reward for LT: 12.54\n",
      "Best Epoch based on MT's Immediate Reward: 0\n",
      "Highest Immediate Reward for MT: 0.74\n",
      "Best Epoch based on ST's Immediate Reward: 45\n",
      "Highest Immediate Reward for ST: 1.87\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluating the trained model\n",
    "print('\\n')\n",
    "print(\"Best Epoch based on Cumulative Reward: \",\n",
    "      np.argmax(cumulative_reward_per_epoch))\n",
    "print(\n",
    "    f\"Highest Cumulative Reward: {round(np.amax(cumulative_reward_per_epoch), 2)}\"\n",
    ")\n",
    "print(f\"Best Epoch based on Global State: {np.argmin(global_state_per_epoch)}\")\n",
    "print(f\"Lowest Global State: {round(np.min(global_state_per_epoch), 2)}\")\n",
    "print('\\n')\n",
    "print(\n",
    "    f\"Best Epoch based on LT's Immediate Reward: {np.argmax(immediate_rewards_per_epoch[:, 0])}\"\n",
    ")\n",
    "print(\n",
    "    f\"Highest Immediate Reward for LT: {round(np.max(immediate_rewards_per_epoch[:, 0]), 2)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Best Epoch based on MT's Immediate Reward: {np.argmax(immediate_rewards_per_epoch[:, 1])}\"\n",
    ")\n",
    "print(\n",
    "    f\"Highest Immediate Reward for MT: {round(np.max(immediate_rewards_per_epoch[:, 1]), 2)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Best Epoch based on ST's Immediate Reward: {np.argmax(immediate_rewards_per_epoch[:, 2])}\"\n",
    ")\n",
    "print(\n",
    "    f\"Highest Immediate Reward for ST: {round(np.max(immediate_rewards_per_epoch[:, 2]), 2)}\"\n",
    ")\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LT's Strategy to achieve Highest Cumulative Reward: \n",
      " [0.101, 0.101, 0.057, 0.079, 0.079, 0.079, 0.057, 0.079, 0.035, 0.057, 0.079, 0.079, 0.079, 0.079, 0.035, 0.035, 0.101, 0.101, 0.079, 0.079, 0.035, 0.035, 0.035, 0.035, 0.035, 0.035, 0.144, 0.079, 0.079, 0.035]\n",
      "LT's Strategy to achieve Highest Immediate Reward: \n",
      " [0.079, 0.079, 0.079, 0.079, 0.079, 0.101, 0.079, 0.079, 0.079, 0.079, 0.057, 0.079, 0.035, 0.079, 0.079, 0.101, 0.057, 0.079, 0.101, 0.079, 0.035, 0.035, 0.079, 0.035, 0.122, 0.035, 0.035, 0.079, 0.231, 0.035]\n",
      "Selfish Policy of LT, based on LT's Final Q-Table: \n",
      " [0.166, 0.122, 0.079, 0.079, 0.079, 0.079, 0.079, 0.101, 0.057, 0.079, 0.079, 0.057, 0.079, 0.122, 0.122, 0.079, 0.035, 0.101, 0.101, 0.057, 0.079, 0.035, 0.101, 0.101, 0.101, 0.144, 0.035, 0.101, 0.144, 0.231, 0.035]\n",
      "\n",
      "\n",
      "MT's Strategy to achieve Highest Cumulative Reward: \n",
      " [0.066, 0.066, 0.08, 0.073, 0.036, 0.015, 0.051, 0.036, 0.036, 0.015, 0.036, 0.036, 0.036, 0.029, 0.015, 0.015, 0.036, 0.015, 0.044, 0.015, 0.044, 0.015, 0.015, 0.015, 0.015, 0.015, 0.051, 0.015, 0.015, 0.015]\n",
      "MT's Strategy to achieve Highest Immediate Reward: \n",
      " [0.015, 0.015, 0.022, 0.073, 0.015, 0.015, 0.015, 0.029, 0.015, 0.015, 0.036, 0.036, 0.015, 0.015, 0.015, 0.015, 0.015, 0.015, 0.015, 0.015, 0.015, 0.015, 0.036, 0.015, 0.015, 0.015, 0.015, 0.015, 0.015, 0.015]\n",
      "Selfish Policy of MT, based on MT's Final Q-Table: \n",
      " [0.051, 0.051, 0.08, 0.08, 0.044, 0.051, 0.044, 0.036, 0.015, 0.044, 0.036, 0.015, 0.015, 0.036, 0.044, 0.015, 0.015, 0.036, 0.015, 0.036, 0.015, 0.015, 0.044, 0.015, 0.015, 0.015, 0.051, 0.015, 0.015, 0.015, 0.015]\n",
      "\n",
      "\n",
      "ST's Strategy to achieve Highest Cumulative Reward: \n",
      " [0.036, 0.029, 0.022, 0.008, 0.022, 0.022, 0.015, 0.022, 0.008, 0.029, 0.022, 0.008, 0.022, 0.008, 0.008, 0.029, 0.008, 0.022, 0.029, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008]\n",
      "ST's Strategy to achieve Highest Immediate Reward: \n",
      " [0.029, 0.071, 0.05, 0.029, 0.022, 0.008, 0.022, 0.022, 0.029, 0.022, 0.008, 0.008, 0.008, 0.008, 0.008, 0.022, 0.029, 0.029, 0.022, 0.008, 0.008, 0.008, 0.008, 0.029, 0.008, 0.008, 0.008, 0.029, 0.036, 0.008]\n",
      "Selfish Policy of ST, based on ST's Final Q-Table: \n",
      " [0.029, 0.029, 0.029, 0.071, 0.022, 0.022, 0.022, 0.022, 0.022, 0.029, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.029, 0.008, 0.029, 0.008, 0.008, 0.008, 0.008, 0.008, 0.029, 0.008, 0.008, 0.008, 0.029, 0.036, 0.008]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#build utilitarian, selfish and greedy policies of each agent\n",
    "Q_LT_Best = Q_LT_per_epoch[np.argmax(cumulative_reward_per_epoch)]\n",
    "Q_MT_Best = Q_MT_per_epoch[np.argmax(cumulative_reward_per_epoch)]\n",
    "Q_ST_Best = Q_ST_per_epoch[np.argmax(cumulative_reward_per_epoch)]\n",
    "\n",
    "Q_LT_Immediate_Best = Q_LT_per_epoch[np.argmax(immediate_rewards_per_epoch[:, 0])]\n",
    "Q_MT_Immediate_Best = Q_MT_per_epoch[np.argmax(immediate_rewards_per_epoch[:, 1])]\n",
    "Q_ST_Immediate_Best = Q_ST_per_epoch[np.argmax(immediate_rewards_per_epoch[:, 2])]\n",
    "                                                       \n",
    "LT_Strategy = [LT_action_space[i] for i in Q_LT_Best[1:]]\n",
    "MT_Strategy = [MT_action_space[i] for i in Q_MT_Best[1:]]\n",
    "ST_Strategy = [ST_action_space[i] for i in Q_ST_Best[1:]]\n",
    "\n",
    "LT_Greedy_Strategy = [LT_action_space[i] for i in Q_LT_Immediate_Best[1:]]\n",
    "MT_Greedy_Strategy = [MT_action_space[i] for i in Q_MT_Immediate_Best[1:]]\n",
    "ST_Greedy_Strategy = [ST_action_space[i] for i in Q_ST_Immediate_Best[1:]]\n",
    "\n",
    "LT_Policy = [LT_action_space[i] for i in np.argmax(Q_LT, axis=1)]\n",
    "MT_Policy = [MT_action_space[i] for i in np.argmax(Q_MT, axis=1)]\n",
    "ST_Policy = [ST_action_space[i] for i in np.argmax(Q_ST, axis=1)]\n",
    "\n",
    "print(f\"LT's Strategy to achieve Highest Cumulative Reward: \\n {LT_Strategy}\")\n",
    "print(f\"LT's Strategy to achieve Highest Immediate Reward: \\n {LT_Greedy_Strategy}\")\n",
    "print(f\"Selfish Policy of LT, based on LT's Final Q-Table: \\n {LT_Policy}\")\n",
    "print('\\n')\n",
    "\n",
    "print(f\"MT's Strategy to achieve Highest Cumulative Reward: \\n {MT_Strategy}\")\n",
    "print(f\"MT's Strategy to achieve Highest Immediate Reward: \\n {MT_Greedy_Strategy}\")\n",
    "print(f\"Selfish Policy of MT, based on MT's Final Q-Table: \\n {MT_Policy}\")\n",
    "print('\\n')\n",
    "\n",
    "print(f\"ST's Strategy to achieve Highest Cumulative Reward: \\n {ST_Strategy}\")\n",
    "print(f\"ST's Strategy to achieve Highest Immediate Reward: \\n {ST_Greedy_Strategy}\")\n",
    "print(f\"Selfish Policy of ST, based on ST's Final Q-Table: \\n {ST_Policy}\")\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_rowcount(Q_Table):\n",
    "  subcount = 1\n",
    "  for row in Q_Table:\n",
    "    count = f\"p{subcount}\"\n",
    "    print(count, row)\n",
    "    subcount += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q-Table of LT: \n",
      " [0.035 0.057 0.079 0.101 0.122 0.144 0.166 0.188 0.209 0.231]\n",
      "p1 [2.124 1.931 2.209 2.128 0.    0.    2.373 0.    0.    0.   ]\n",
      "p2 [2.499 2.581 2.58  1.495 2.631 0.    0.    0.    0.    0.   ]\n",
      "p3 [2.125 2.014 2.551 0.98  0.    0.    0.    0.    0.    0.   ]\n",
      "p4 [2.959 2.906 3.265 1.984 2.494 0.    0.    0.    0.    0.   ]\n",
      "p5 [2.717 2.231 3.028 2.393 0.    0.    0.    0.    0.    0.   ]\n",
      "p6 [2.349 2.584 2.672 0.    0.    0.    0.    0.    0.    0.   ]\n",
      "p7 [2.334 2.549 2.667 1.684 0.    0.    0.    0.    0.    0.   ]\n",
      "p8 [2.361 2.27  2.546 2.636 0.    0.    0.    0.    0.    0.   ]\n",
      "p9 [3.364 3.589 3.493 0.    0.    0.    0.    0.    0.    0.   ]\n",
      "p10 [3.383 3.268 3.645 0.    0.    0.    0.    0.    0.    0.   ]\n",
      "p11 [3.467 3.377 3.521 3.223 0.    0.    0.    0.    0.    0.   ]\n",
      "p12 [3.419 3.436 2.864 2.81  0.    0.    0.    0.    0.    0.   ]\n",
      "p13 [3.65  2.791 3.812 2.922 1.045 0.    0.    0.    0.    0.   ]\n",
      "p14 [3.334 3.535 3.628 0.    3.788 0.    0.    0.    0.    0.   ]\n",
      "p15 [3.362 0.    3.562 2.687 3.624 0.    0.    0.    0.    0.   ]\n",
      "p16 [3.411 3.025 3.729 0.    0.    0.    0.    0.    0.    0.   ]\n",
      "p17 [3.647 1.382 3.49  2.836 0.    0.    0.    0.    0.    0.   ]\n",
      "p18 [3.573 3.409 3.437 3.721 0.    0.    0.    0.    0.    0.   ]\n",
      "p19 [3.482 2.375 3.561 3.638 0.    0.    0.    0.    0.    0.   ]\n",
      "p20 [3.773 3.913 3.896 3.519 0.    0.    0.    0.    0.    0.   ]\n",
      "p21 [3.824 0.    4.075 3.262 1.444 0.    0.    0.    0.    0.   ]\n",
      "p22 [4.031 0.    3.264 2.556 4.024 0.    0.    0.    0.    0.   ]\n",
      "p23 [3.974 0.    3.666 3.976 0.    0.    0.    0.    0.    0.   ]\n",
      "p24 [4.06  0.    3.769 4.372 0.    0.    0.    0.    0.    0.   ]\n",
      "p25 [3.958 2.824 3.956 4.009 0.    3.684 0.    0.    0.    0.   ]\n",
      "p26 [3.937 0.    3.06  2.366 2.807 4.101 0.    0.    0.    0.   ]\n",
      "p27 [3.831 3.645 3.639 3.668 0.    0.    0.    0.    0.    0.   ]\n",
      "p28 [3.492 3.619 2.805 3.735 2.453 2.119 0.    0.    0.    0.   ]\n",
      "p29 [3.257 3.322 2.503 2.229 0.    3.626 0.    0.    0.    0.   ]\n",
      "p30 [1.706 0.    1.828 1.951 0.    0.    0.    0.    0.    2.52 ]\n",
      "None\n",
      "\n",
      "\n",
      "Final Q-Table of MT: \n",
      " [0.015 0.022 0.029 0.036 0.044 0.051 0.058 0.066 0.073 0.08 ]\n",
      "p1 [1.047 1.144 0.884 1.21  1.233 1.269 1.197 1.079 0.    0.   ]\n",
      "p2 [1.224 1.227 1.268 1.297 1.199 1.303 0.    0.829 0.    0.   ]\n",
      "p3 [1.491 1.456 1.544 1.544 1.532 1.038 0.874 0.956 0.    1.809]\n",
      "p4 [1.472 1.032 1.415 0.988 1.553 1.539 0.    0.73  0.    1.693]\n",
      "p5 [1.547 0.937 1.244 1.611 1.637 0.    0.594 0.    0.877 0.   ]\n",
      "p6 [1.496 1.172 1.516 1.631 1.329 1.665 0.    0.    0.    0.   ]\n",
      "p7 [1.501 1.571 1.528 1.637 1.675 0.    0.    0.    0.    0.   ]\n",
      "p8 [1.609 1.425 1.53  1.688 1.022 1.05  0.    0.    0.    0.   ]\n",
      "p9 [1.694 1.669 1.489 1.576 1.589 0.    0.    0.    0.    0.   ]\n",
      "p10 [1.709 0.    1.761 1.828 1.877 0.    0.    0.    0.    0.   ]\n",
      "p11 [1.732 0.    1.618 1.835 0.844 0.    0.    0.    0.    0.   ]\n",
      "p12 [1.738 0.    1.705 1.684 1.104 0.    0.    0.    0.    0.   ]\n",
      "p13 [1.8   1.466 1.67  0.975 1.381 0.    0.    0.739 0.    0.   ]\n",
      "p14 [1.76  1.533 1.811 1.867 1.596 0.    0.    0.    0.    0.   ]\n",
      "p15 [1.713 1.766 1.518 1.619 1.831 0.    0.    0.    0.    0.   ]\n",
      "p16 [1.813 0.    1.734 0.798 1.718 0.    0.    0.    0.    0.   ]\n",
      "p17 [1.792 0.    1.329 1.763 0.315 0.    0.    0.    0.    0.   ]\n",
      "p18 [1.806 0.    1.899 1.932 0.    0.    0.    0.    0.    0.   ]\n",
      "p19 [1.85  0.    1.349 0.    1.728 0.    0.    0.    0.    0.   ]\n",
      "p20 [1.813 0.    0.    1.912 1.827 0.    0.    0.    0.    0.   ]\n",
      "p21 [1.822 0.286 1.416 0.    1.801 0.    0.    0.    0.    0.   ]\n",
      "p22 [1.807 0.    1.311 1.756 1.381 1.625 0.    0.    0.    0.   ]\n",
      "p23 [1.811 0.    0.    0.    1.929 0.    0.    0.    0.    0.   ]\n",
      "p24 [1.797 1.053 0.    1.78  0.    0.    0.    0.    0.    0.   ]\n",
      "p25 [1.803 0.    0.831 1.644 1.596 0.    0.    0.    0.    0.   ]\n",
      "p26 [1.808 0.    0.    0.    0.446 1.454 0.    0.701 0.    0.   ]\n",
      "p27 [1.677 0.    0.    0.    0.319 1.837 0.    1.684 0.    0.   ]\n",
      "p28 [1.518 0.    0.    0.    0.    1.437 0.    0.    0.    0.   ]\n",
      "p29 [1.221 0.    0.    1.128 0.    0.    0.    0.    0.    0.   ]\n",
      "p30 [0.735 0.    0.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      "None\n",
      "\n",
      "\n",
      "Final Q-Table of ST: \n",
      " [0.008 0.015 0.022 0.029 0.036 0.043 0.05  0.057 0.064 0.071]\n",
      "p1 [0.619 0.585 0.645 0.657 0.59  0.    0.    0.    0.    0.   ]\n",
      "p2 [0.896 0.965 0.964 0.996 0.697 0.814 0.    0.    0.    0.   ]\n",
      "p3 [0.835 0.904 0.867 0.981 0.    0.803 0.    0.    0.    0.884]\n",
      "p4 [0.823 0.819 0.85  0.845 0.75  0.631 0.976 0.    0.    1.098]\n",
      "p5 [0.936 0.687 0.991 0.99  0.    0.    0.    0.    0.359 0.   ]\n",
      "p6 [0.922 0.725 1.02  0.424 0.2   0.    0.    0.    0.    0.   ]\n",
      "p7 [0.945 0.158 0.994 0.866 0.    0.    0.    0.    0.    0.   ]\n",
      "p8 [0.981 0.491 1.05  0.284 0.    0.    0.    0.    0.    0.   ]\n",
      "p9 [1.002 0.157 1.031 0.    0.    0.    0.    0.    0.    0.   ]\n",
      "p10 [0.992 0.079 0.964 1.073 0.    0.    0.    0.    0.    0.   ]\n",
      "p11 [0.984 0.    0.983 0.547 0.    0.    0.    0.    0.    0.   ]\n",
      "p12 [0.996 0.    0.946 0.    0.    0.    0.    0.    0.    0.   ]\n",
      "p13 [0.993 0.    0.361 0.    0.    0.    0.    0.    0.    0.   ]\n",
      "p14 [0.98  0.913 0.849 0.836 0.    0.    0.    0.    0.    0.   ]\n",
      "p15 [0.989 0.    0.897 0.    0.    0.    0.    0.    0.    0.   ]\n",
      "p16 [1.    0.    0.    0.895 0.    0.    0.    0.    0.    0.   ]\n",
      "p17 [0.982 0.    1.033 1.052 0.    0.    0.    0.    0.    0.   ]\n",
      "p18 [0.993 0.    0.935 0.961 0.    0.    0.    0.    0.    0.   ]\n",
      "p19 [0.983 0.    0.566 1.025 0.    0.    0.    0.    0.    0.   ]\n",
      "p20 [0.983 0.    0.955 0.553 0.    0.    0.    0.    0.    0.   ]\n",
      "p21 [0.99 0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      "p22 [0.985 0.    0.853 0.    0.    0.    0.    0.    0.    0.   ]\n",
      "p23 [0.983 0.    0.    0.883 0.    0.    0.    0.    0.    0.   ]\n",
      "p24 [0.992 0.    0.    0.851 0.    0.    0.    0.    0.    0.   ]\n",
      "p25 [0.963 0.    0.    0.974 0.    0.    0.    0.    0.    0.   ]\n",
      "p26 [0.947 0.    0.    0.    0.    0.398 0.    0.    0.    0.   ]\n",
      "p27 [0.9   0.    0.    0.    0.828 0.    0.    0.    0.    0.   ]\n",
      "p28 [0.839 0.    0.457 0.    0.    0.    0.    0.    0.    0.   ]\n",
      "p29 [0.676 0.    0.    0.723 0.423 0.    0.    0.    0.    0.   ]\n",
      "p30 [0.39  0.    0.    0.    0.517 0.    0.    0.    0.    0.   ]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final Q-Table of LT: \\n {LT_action_space}\")  #Q-Tables are printed...\n",
    "print(Q_rowcount(Q_LT[:years]))\n",
    "print('\\n')\n",
    "\n",
    "print(f\"Final Q-Table of MT: \\n {MT_action_space}\")  #Q-Tables are printed...\n",
    "print(Q_rowcount(Q_MT[:years]))\n",
    "print('\\n')\n",
    "\n",
    "print(\n",
    "    f\"Final Q-Table of ST: \\n {ST_action_space}\"\n",
    ")  #...Based on the \"rewards_per_epoch\" Table, the best Q-Tables are identified as policies\n",
    "print(Q_rowcount(Q_ST[:years]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Cumulative Reward under Selfish Policies: 160.5\n",
      "Global State after using Selfish Policies: 4.67\n",
      "\n",
      "\n",
      "Percentage of Lower Cumulative Reward comparing Selfish Policies to Best Epoch: 51.21%\n",
      "Percentage of Higher Global State comparing Selfish Policies to Best Epoch: 5.25%\n"
     ]
    }
   ],
   "source": [
    "#Under utilary Policies\n",
    "#initializing variables which are reset for every new epoch\n",
    "global_state_of_co2_emission = 4.025  #current CO2 emissions are at 4.97 metric tons per capita world-wide. Source: World Bank\n",
    "cumulative_reward_selfish = 0  #initializing cumulative reward, which is 0 to start with\n",
    "alpha = 0.1  #initializing the learning rate of the Q-values\n",
    "Q_LT = np.zeros((years+1, size_of_action_space))\n",
    "Q_MT = np.zeros((years+1, size_of_action_space))\n",
    "Q_ST = np.zeros((years+1, size_of_action_space))\n",
    "\n",
    "#run period loop\n",
    "for year in range(0,year):\n",
    "\n",
    "    #actions based on Policies\n",
    "    LT_action = LT_Strategy[year]\n",
    "    MT_action = MT_Strategy[year]\n",
    "    ST_action = ST_Strategy[year]\n",
    "\n",
    "    #Immediate Reward functions\n",
    "    LT_immediate_reward = -1 * LT_action * LT_reward_factor + cost_of_action * LT_action  #defining immediate reward function of LT per period\n",
    "    MT_immediate_reward = -1 * LT_action * MT_reward_factor + cost_of_action * MT_action  #defining immediate reward function of LT per period\n",
    "    ST_immediate_reward = -1 * ST_action * ST_reward_factor + cost_of_action * ST_action  #defining immediate reward function of ST per period\n",
    "\n",
    "    #to be maximized: += immediate rewards of both agents + the negative of the actions that each agent takes, because the actions influence the global state, which shall be minimized\n",
    "    Q_ST[year, abs(ST_action_space - ST_action).argmin()] = round(\n",
    "            (1 - alpha) * Q_ST[year, Max_Q_ST] + alpha *\n",
    "            (ST_immediate_reward + gamma * np.amax(Q_ST[year + 1, :])), 3)\n",
    "        \n",
    "        #to be maximized: += immediate rewards of both agents + the negative of the actions that each agent takes, because the actions influence the global state, which shall be minimized\n",
    "    cumulative_reward_selfish += LT_immediate_reward + MT_immediate_reward + ST_immediate_reward - cost_of_action*((LT_action_space[np.argmax(Q_LT[year, :])]) - (MT_action_space[np.argmax(Q_MT[year, :])]) - (ST_action_space[np.argmax(Q_ST[year, :])]))\n",
    "        \n",
    "    global_state_of_co2_emission += ((LT_action*Population_LT[year]) + (MT_action*Population_MT[year]) + (ST_action*Population_ST[year]))/Population_array[year]\n",
    "\n",
    "print('\\n')\n",
    "print(\n",
    "    f\"Cumulative Reward under Selfish Policies: {round(cumulative_reward_selfish, 2)}\"\n",
    ")\n",
    "print(f\"Global State after using Selfish Policies: {round(global_state_of_co2_emission, 2)}\")\n",
    "\n",
    "Selfish_Reward_Loss = round(\n",
    "    100 *\n",
    "    (cumulative_reward_selfish - np.max(cumulative_reward_per_epoch)) /\n",
    "    np.max(cumulative_reward_per_epoch), 2)\n",
    "CO2_Selfish = round(\n",
    "    100 * ((global_state_of_co2_emission - np.min(global_state_per_epoch)) /\n",
    "           np.min(global_state_per_epoch)), 2)\n",
    "\n",
    "print('\\n')\n",
    "print(\n",
    "    f\"Percentage of Lower Cumulative Reward comparing Selfish Policies to Best Epoch: {Selfish_Reward_Loss}%\"\n",
    ")\n",
    "print(\n",
    "    f\"Percentage of Higher Global State comparing Selfish Policies to Best Epoch: {CO2_Selfish}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.6650532470385055"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_state_of_co2_emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Cumulative Reward under Selfish Policies: 189.54\n",
      "Global State after using Selfish Policies: 5.4\n",
      "\n",
      "\n",
      "Percentage of Lower Cumulative Reward comparing Utilary Policies to Best Epoch: 51.21%\n",
      "Percentage of Higher Global State comparing Utilary Policies to Best Epoch: 5.25%\n"
     ]
    }
   ],
   "source": [
    "#Under utilary Policies\n",
    "#initializing variables which are reset for every new epoch\n",
    "global_state = 4.025  #current CO2 emissions are at 4.97 metric tons per capita world-wide. Source: World Bank\n",
    "cumulative_reward_Utilary = 0  #initializing cumulative reward, which is 0 to start with\n",
    "alpha = 0.1  #initializing the learning rate of the Q-values\n",
    "Q_LT = np.zeros((years+1, size_of_action_space))\n",
    "Q_MT = np.zeros((years+1, size_of_action_space))\n",
    "Q_ST = np.zeros((years+1, size_of_action_space))\n",
    "\n",
    "#run period loop\n",
    "for year in range(0,year):\n",
    "\n",
    "    #actions based on Policies\n",
    "    LT_action = LT_Policy[year]\n",
    "    MT_action = MT_Policy[year]\n",
    "    ST_action = ST_Policy[year]\n",
    "\n",
    "    #Immediate Reward functions\n",
    "    LT_immediate_reward = -1 * LT_action * LT_reward_factor + cost_of_action * LT_action  #defining immediate reward function of LT per period\n",
    "    MT_immediate_reward = -1 * LT_action * MT_reward_factor + cost_of_action * MT_action  #defining immediate reward function of LT per period\n",
    "    ST_immediate_reward = -1 * ST_action * ST_reward_factor + cost_of_action * ST_action  #defining immediate reward function of ST per period\n",
    "\n",
    "    #to be maximized: += immediate rewards of both agents + the negative of the actions that each agent takes, because the actions influence the global state, which shall be minimized\n",
    "    Q_ST[year, abs(ST_action_space - ST_action).argmin()] = round(\n",
    "            (1 - alpha) * Q_ST[year, Max_Q_ST] + alpha *\n",
    "            (ST_immediate_reward + gamma * np.amax(Q_ST[year + 1, :])), 3)\n",
    "        \n",
    "        #to be maximized: += immediate rewards of both agents + the negative of the actions that each agent takes, because the actions influence the global state, which shall be minimized\n",
    "    cumulative_reward_Utilary += LT_immediate_reward + MT_immediate_reward + ST_immediate_reward - cost_of_action*((LT_action_space[np.argmax(Q_LT[year, :])]) - (MT_action_space[np.argmax(Q_MT[year, :])]) - (ST_action_space[np.argmax(Q_ST[year, :])]))\n",
    "        \n",
    "    global_state_of_co2_emission += ((LT_action*Population_LT[year]) + (MT_action*Population_MT[year]) + (ST_action*Population_ST[year]))/Population_array[year]\n",
    "\n",
    "print('\\n')\n",
    "print(\n",
    "    f\"Cumulative Reward under Selfish Policies: {round(cumulative_reward_Utilary, 2)}\"\n",
    ")\n",
    "print(f\"Global State after using Selfish Policies: {round(global_state_of_co2_emission, 2)}\")\n",
    "\n",
    "Utilary_Reward_Loss = round(\n",
    "    100 *\n",
    "    (cumulative_reward_selfish - np.max(cumulative_reward_per_epoch)) /\n",
    "    np.max(cumulative_reward_per_epoch), 2)\n",
    "CO2_Utilary = round(\n",
    "    100 * ((global_state - np.min(global_state_per_epoch)) /\n",
    "           np.min(global_state_per_epoch)), 2)\n",
    "\n",
    "print('\\n')\n",
    "print(\n",
    "    f\"Percentage of Lower Cumulative Reward comparing Utilary Policies to Best Epoch: {Selfish_Reward_Loss}%\"\n",
    ")\n",
    "print(\n",
    "    f\"Percentage of Higher Global State comparing Utilary Policies to Best Epoch: {CO2_Selfish}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Cumulative Reward under Selfish Policies: 148.09\n",
      "Global State after using Selfish Policies: 6.07\n",
      "\n",
      "\n",
      "Percentage of Lower Cumulative Reward comparing Utilary Policies to Best Epoch: 51.21%\n",
      "Percentage of Higher Global State comparing Utilary Policies to Best Epoch: 5.25%\n"
     ]
    }
   ],
   "source": [
    "#Under utilary Policies\n",
    "#initializing variables which are reset for every new epoch\n",
    "global_state = 4.025  #current CO2 emissions are at 4.97 metric tons per capita world-wide. Source: World Bank\n",
    "cumulative_reward_Utilary = 0  #initializing cumulative reward, which is 0 to start with\n",
    "alpha = 0.1  #initializing the learning rate of the Q-values\n",
    "Q_LT = np.zeros((years+1, size_of_action_space))\n",
    "Q_MT = np.zeros((years+1, size_of_action_space))\n",
    "Q_ST = np.zeros((years+1, size_of_action_space))\n",
    "\n",
    "#run period loop\n",
    "for year in range(0,year):\n",
    "\n",
    "    #actions based on Policies\n",
    "    LT_action = LT_Greedy_Strategy[year]\n",
    "    MT_action = MT_Greedy_Strategy[year]\n",
    "    ST_action = ST_Greedy_Strategy[year]\n",
    "\n",
    "    #Immediate Reward functions\n",
    "    LT_immediate_reward = -1 * LT_action * LT_reward_factor + cost_of_action * LT_action  #defining immediate reward function of LT per period\n",
    "    MT_immediate_reward = -1 * LT_action * MT_reward_factor + cost_of_action * MT_action  #defining immediate reward function of LT per period\n",
    "    ST_immediate_reward = -1 * ST_action * ST_reward_factor + cost_of_action * ST_action  #defining immediate reward function of ST per period\n",
    "\n",
    "    #to be maximized: += immediate rewards of both agents + the negative of the actions that each agent takes, because the actions influence the global state, which shall be minimized\n",
    "    Q_ST[year, abs(ST_action_space - ST_action).argmin()] = round(\n",
    "            (1 - alpha) * Q_ST[year, Max_Q_ST] + alpha *\n",
    "            (ST_immediate_reward + gamma * np.amax(Q_ST[year + 1, :])), 3)\n",
    "        \n",
    "        #to be maximized: += immediate rewards of both agents + the negative of the actions that each agent takes, because the actions influence the global state, which shall be minimized\n",
    "    cumulative_reward_Utilary += LT_immediate_reward + MT_immediate_reward + ST_immediate_reward - cost_of_action*((LT_action_space[np.argmax(Q_LT[year, :])]) - (MT_action_space[np.argmax(Q_MT[year, :])]) - (ST_action_space[np.argmax(Q_ST[year, :])]))\n",
    "        \n",
    "    global_state_of_co2_emission += ((LT_action*Population_LT[year]) + (MT_action*Population_MT[year]) + (ST_action*Population_ST[year]))/Population_array[year]\n",
    "\n",
    "print('\\n')\n",
    "print(\n",
    "    f\"Cumulative Reward under Selfish Policies: {round(cumulative_reward_Utilary, 2)}\"\n",
    ")\n",
    "print(f\"Global State after using Selfish Policies: {round(global_state_of_co2_emission, 2)}\")\n",
    "\n",
    "Utilary_Reward_Loss = round(\n",
    "    100 *\n",
    "    (cumulative_reward_selfish - np.max(cumulative_reward_per_epoch)) /\n",
    "    np.max(cumulative_reward_per_epoch), 2)\n",
    "CO2_Utilary = round(\n",
    "    100 * ((global_state - np.min(global_state_per_epoch)) /\n",
    "           np.min(global_state_per_epoch)), 2)\n",
    "\n",
    "print('\\n')\n",
    "print(\n",
    "    f\"Percentage of Lower Cumulative Reward comparing Utilary Policies to Best Epoch: {Selfish_Reward_Loss}%\"\n",
    ")\n",
    "print(\n",
    "    f\"Percentage of Higher Global State comparing Utilary Policies to Best Epoch: {CO2_Selfish}%\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Dynamic Paper_MARL & Global Carbon Footprint_School of AI.ipynb.ipynb",
   "provenance": [
    {
     "file_id": "1wOS8w3V6DoSO-c46Pbq-OUr7kQl8cFl9",
     "timestamp": 1568473104371
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "558.206px",
    "left": "974.245px",
    "right": "20px",
    "top": "87.9884px",
    "width": "427.488px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
